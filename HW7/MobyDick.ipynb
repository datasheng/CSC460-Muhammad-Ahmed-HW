{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "3"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 1. Tools for text processing\n",
    "<p><img style=\"float: right ; margin: 5px 20px 5px 10px; width: 45%\" src=\"https://assets.datacamp.com/production/project_38/img/Moby_Dick_p510_illustration.jpg\"> </p>\n",
    "<p>What are the most frequent words in Herman Melville's novel, Moby Dick, and how often do they occur?</p>\n",
    "<p>In this notebook, we'll scrape the novel <em>Moby Dick</em> from the website <a href=\"https://www.gutenberg.org/\">Project Gutenberg</a> (which contains a large corpus of books) using the Python package <code>requests</code>. Then we'll extract words from this web data using <code>BeautifulSoup</code>. Finally, we'll dive into analyzing the distribution of words using the Natural Language ToolKit (<code>nltk</code>) and <code>Counter</code>.</p>\n",
    "<p>The <em>Data Science pipeline</em> we'll build in this notebook can be used to visualize the word frequency distributions of any novel that you can find on Project Gutenberg. The natural language processing tools used here apply to much of the data that data scientists encounter as a vast proportion of the world's data is unstructured data and includes a great deal of text.</p>\n",
    "<p>Let's start by loading in the three main Python packages we are going to use.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "dc": {
     "key": "3"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importing requests, BeautifulSoup, nltk, and Counter\n",
    "# ... YOUR CODE FOR TASK 1 ...\n",
    "# Load the three Python packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "10"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 2. Request Moby Dick\n",
    "<p>To analyze Moby Dick, we need to get the contents of Moby Dick from <em>somewhere</em>. Luckily, the text is freely available online at Project Gutenberg as an HTML file: https://www.gutenberg.org/files/2701/2701-h/2701-h.htm .</p>\n",
    "<p><strong>Note</strong> that HTML stands for Hypertext Markup Language and is the standard markup language for the web.</p>\n",
    "<p>To fetch the HTML file with Moby Dick we're going to use the <code>request</code> package to make a <code>GET</code> request for the website, which means we're <em>getting</em> data from it. This is what you're doing through a browser when visiting a webpage, but now we're getting the requested page directly into Python instead. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dc": {
     "key": "10"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "\n",
      "<!DOCTYPE html\n",
      "   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n",
      "   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\n",
      "\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
      "  <head>\n",
      "    <title>\n",
      "      Moby Dick; Or the Whale, by Herman Melville\n",
      "    </title>\n",
      "    <style type=\"text/css\" xml:space=\"preserve\">\n",
      "\n",
      "    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\n",
      "    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n",
      "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n",
      "    hr  { width: 50%; text-align: center;}\n",
      "    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\n",
      "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n",
      "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n",
      "    .toc       { margin-left: 10%; margin-bottom: .75em;}\n",
      "    .toc2      { margin-left: 20%;}\n",
      "    div.fig    { display:block; margin:0 auto; text-align:center; }\n",
      "    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\n",
      "    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\n",
      "    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\n",
      "    .pagenum   {display:inline; font-size: 70%; font-style:normal;\n",
      "               margin: 0; padding: 0; position: absolute; right: 1%;\n",
      "               text-align: right;}\n",
      "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n",
      "\n",
      "    table      {margin-left: 10%;}\n",
      "\n",
      "a:link {color:blue;\n",
      "\t\ttext-decoration:none}\n",
      "link {color:blue;\n",
      "\t\ttext-decoration:none}\n",
      "a:visited {color:blue;\n",
      "\t\ttext-decoration:none}\n",
      "a:hover {color:red}\n",
      "\n",
      "</style>\n",
      "  </head>\n",
      "  <body>\n",
      "<pre xml:space=\"preserve\">\n",
      "\n",
      "The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\n",
      "\n",
      "This eBook is for the use of anyone anywh\n"
     ]
    }
   ],
   "source": [
    "# 2. Request Moby Dick\n",
    "\n",
    "# Import the requests package\n",
    "import requests\n",
    "\n",
    "# Fetch the HTML content of Moby Dick from the provided S3 link\n",
    "r = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n",
    "\n",
    "# Set the correct encoding\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "# Extract the HTML content as text\n",
    "html = r.text\n",
    "\n",
    "# Print the first 2000 characters to inspect\n",
    "print(html[:2000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "17"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 3. Get the text from the HTML\n",
    "<p>This HTML is not quite what we want. However, it does <em>contain</em> what we want: the text of <em>Moby Dick</em>. What we need to do now is <em>wrangle</em> this HTML to extract the text of the novel. For this we'll use the package <code>BeautifulSoup</code>.</p>\n",
    "<p>Firstly, a word on the name of the package: Beautiful Soup? In web development, the term \"tag soup\" refers to structurally or syntactically incorrect HTML code written for a web page. What Beautiful Soup does best is to make tag soup beautiful again and to extract information from it with ease! In fact, the main object created and queried when using this package is called <code>BeautifulSoup</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dc": {
     "key": "17"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent me\n",
      "      from deliberately stepping into the street, and methodically knocking\n",
      "      people’s hats off—then, I account it high time to get to sea as soon\n",
      "      as I can. This is my substitute for pistol and ball. With a philosophical\n",
      "      flourish Cato throws himself upon his sword; I quietly take to the ship.\n",
      "      There is nothing surprising in this. If they but knew it, almost all men\n",
      "      in their degree, some time or other, cherish very nearly the same feelings\n",
      "      towards the ocean with me.\n",
      "    \n",
      "\n",
      "      There now is your insular city of the Manhattoes, belted round by wharves\n",
      "      as Indian isles by coral reefs—commerce surrounds it with her surf.\n",
      "      Right and left, the streets take you waterward. Its extreme downtown is\n",
      "      the battery, where that noble mole is washed by waves, and cooled by\n",
      "      breezes, which a few hours previous were out of sight of land. Look at the\n",
      "      crowds of water-gazers there.\n",
      "    \n",
      "\n",
      "      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\n",
      "      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\n",
      "      you see?—Posted like silent sentinels all around the town, stand\n",
      "      thousands upon thousands of mortal men fixed in ocean reveries. Some\n",
      "      leaning against the spiles; some seated upon the pier-heads; some looking\n",
      "      over the bulwarks of ships from China; some high aloft in the rigging, as\n",
      "      if striving to get a still better seaward peep. But these are all\n",
      "      landsmen; of week days pent up in lath and plaster—tied to counters,\n",
      "      nailed to benches, clinched to desks. How then is this? Are the green\n",
      "      fields gone? What do they here?\n",
      "    \n",
      "\n",
      "      But look! here come more crowds, pacing straight for the water, and\n",
      "      seemingly bound for a dive. Strange! Nothing will content them but the\n",
      "      extremest limit of the land; loitering under the shady lee of yonder\n",
      "      warehouses will not suffice. No. They must get just as nigh th\n"
     ]
    }
   ],
   "source": [
    "# Creating a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Getting the text out of the soup\n",
    "text = soup.get_text()\n",
    "\n",
    "# Printing out text between characters 32000 and 34000\n",
    "print(text[32000:34000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "24"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 4. Extract the words\n",
    "<p>We now have the text of the novel! There is some unwanted stuff at the start and some unwanted stuff at the end. We could remove it, but this content is so much smaller in amount than the text of Moby Dick that, to a first approximation, it is okay to leave it in.</p>\n",
    "<p>Now that we have the text of interest, it's time to count how many times each word appears, and for this we'll use <code>nltk</code> – the Natural Language Toolkit. We'll start by tokenizing the text, that is, remove everything that isn't a word (whitespace, punctuation, etc.) and then split the text into a list of words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dc": {
     "key": "24"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Moby', 'Dick', 'Or', 'the', 'Whale', 'by', 'Herman', 'Melville']\n"
     ]
    }
   ],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "print(tokens[:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 5. Make the words lowercase\n",
    "<p>OK! We're nearly there. Note that in the above 'Or' has a capital 'O' and that in other places it may not, but both 'Or' and 'or' should be counted as the same word. For this reason, we should build a list of all words in <em>Moby Dick</em> in which all capital letters have been made lower case.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dc": {
     "key": "31"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
     ]
    }
   ],
   "source": [
    "# Create a list called words containing all tokens transformed to lower-case\n",
    "words = [word.lower() for word in tokens]\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "print(words[:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "38"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 6. Load in stop words\n",
    "<p>It is common practice to remove words that appear a lot in the English language such as 'the', 'of' and 'a' because they're not so interesting. Such words are known as <em>stop words</em>. The package <code>nltk</code> includes a good list of stop words in English that we can use.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "38"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/edgemaster/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/share/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/python3.13/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/edgemaster/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/share/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Getting the English stop words from nltk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sw = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Printing out the first eight stop words\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(sw[:\u001b[32m8\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/python3.13/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/python3.13/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/python3.13/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/edgemaster/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/share/nltk_data'\n    - '/Users/edgemaster/csc460/CSC460-Muhammad-Ahmed-HW/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Getting the English stop words from nltk\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "print(sw[:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 7. Remove stop words in Moby Dick\n",
    "<p>We now want to create a new list with all <code>words</code> in Moby Dick, except those that are stop words (that is, those words listed in <code>sw</code>).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "45"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a list words_ns containing all words that are in words but not in sw\n",
    "words_ns = [word for word in words if word not in sw]\n",
    "\n",
    "# Printing the first 5 words_ns to check that stop words are gone\n",
    "print(words_ns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 8. We have the answer\n",
    "<p>Our original question was:</p>\n",
    "<blockquote>\n",
    "  <p>What are the most frequent words in Herman Melville's novel Moby Dick and how often do they occur?</p>\n",
    "</blockquote>\n",
    "<p>We are now ready to answer that! Let's answer this question using the <code>Counter</code> class we imported earlier.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "52"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8. We have the answer\n",
    "\n",
    "# Import Counter from collections\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize a Counter object from our processed list of words without stopwords\n",
    "count = Counter(words_ns)\n",
    "\n",
    "# Store the 10 most common words and their counts\n",
    "top_ten = count.most_common(10)\n",
    "\n",
    "# Print the top ten words and their counts\n",
    "print(top_ten)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "59"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 9. The most common word\n",
    "<p>Nice! Using our variable <code>top_ten</code>, we now have an answer to our original question.</p>\n",
    "<p>The natural language processing skills we used in this notebook are also applicable to much of the data that Data Scientists encounter as the vast proportion of the world's data is unstructured data and includes a great deal of text. </p>\n",
    "<p>So, what word turned out to (<em>not surprisingly</em>) be the most common word in Moby Dick?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "59"
    },
    "tags": [
     "sample_code"
    ],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the most common word from top_ten\n",
    "most_common_word = top_ten[0][0]\n",
    "\n",
    "# Print the most common word\n",
    "print(most_common_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
